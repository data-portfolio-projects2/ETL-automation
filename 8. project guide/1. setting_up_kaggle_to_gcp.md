## Setting up Google Cloud SDK and Migrating Data Pipeline to GCP

This document outlines the process of migrating a data pipeline from local storage to Google Cloud Storage (GCS) and integrating it with Google Cloud Platform (GCP) for processing.

### Key Changes:

*   **Google Cloud Storage Integration:** The `google.cloud.storage` client is used to upload the dataset to GCS.
*   **Authentication:** The script authenticates with GCP using a service account credentials file.
*   **Upload Dataset to GCS:** The dataset file is uploaded to the GCP bucket after itâ€™s downloaded from Kaggle.
*   **Docker and GCP Processing:** The process is being set up to eventually use Docker for processing and will be deployed via Cloud Functions or Cloud Run.

### Python Script

Here's the updated Python script incorporating GCS and GCP:

```python
import os
from kaggle.api.kaggle_api_extended import KaggleApi
from google.cloud import storage

# Set up environment variables for GCP
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "path_to_your_gcp_service_account_key.json"  # Path to your GCP service account key

# Kaggle API setup
os.environ['KAGGLE_CONFIG_DIR'] = 'C:\\folder'  # Path to your Kaggle credentials
api = KaggleApi()
api.authenticate()

# Dataset information
dataset_name = 'adarsh0806/influencer-merchandise-sales'  # The dataset you want to download
download_dir = 'C:\\folder'  # Temporary storage directory for downloaded files
gcs_bucket_name = "your-gcp-bucket-name"  # Your GCP Bucket name
gcs_destination_blob = "datasets/influencer_merchandise_sales/"  # Folder in your GCS bucket

def download_dataset():
    """Downloads the dataset from Kaggle if not already downloaded."""
    # Create download directory if it doesn't exist
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)
    
    # Download the dataset
    print(f"Downloading dataset: {dataset_name}")
    api.dataset_download_files(dataset_name, path=download_dir, unzip=True)
    print("Dataset downloaded successfully!")

def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_blob_name):
    """Uploads a local file to GCS."""
    client = storage.Client()
    bucket = client.get_bucket(gcs_bucket_name)
    blob = bucket.blob(gcs_blob_name)
    
    # Upload the file
    blob.upload_from_filename(local_file_path)
    print(f"File uploaded to GCS: {gcs_blob_name}")

def main():
    """Main function to download from Kaggle and upload to GCS."""
    # Download dataset from Kaggle
    download_dataset()
    
    # After download, upload to GCS (example: upload a specific file from the dataset)
    local_file_path = os.path.join(download_dir, "influencer_merchandise_sales.csv")  # Adjust based on your dataset's filename
    gcs_blob_name = gcs_destination_blob + "influencer_merchandise_sales.csv"
    
    # Upload file to GCS
    upload_to_gcs(local_file_path, gcs_bucket_name, gcs_blob_name)

if __name__ == "__main__":
    main()
```

### Script Explanation:

1.  **Import Libraries:** Imports necessary libraries:
    *   `os`: For interacting with the operating system (environment variables, file paths).
    *   `kaggle.api.kaggle_api_extended.KaggleApi`: For interacting with the Kaggle API.
    *   `google.cloud.storage`: For interacting with Google Cloud Storage.

2.  **Environment Variables Setup:**
    *   `GOOGLE_APPLICATION_CREDENTIALS`: Specifies the path to your GCP service account key JSON file. This is crucial for authentication with GCP.
    *   `KAGGLE_CONFIG_DIR`: Specifies the path to your Kaggle API credentials file.

3.  **Kaggle API Initialization:** Authenticates the Kaggle API using your credentials.

4.  **Dataset Information:**
    *   `dataset_name`: The Kaggle dataset identifier.
    *   `download_dir`: The local directory where the dataset will be temporarily downloaded.
    *   `gcs_bucket_name`: The name of your GCP bucket.
    *   `gcs_destination_blob`: The folder structure within your GCP bucket where the dataset will be uploaded.

5.  **`download_dataset()` Function:**
    *   Creates the `download_dir` if it doesn't exist.
    *   Downloads the dataset from Kaggle using `api.dataset_download_files()`, unzipping the files in the process.

6.  **`upload_to_gcs()` Function:**
    *   Creates a `storage.Client()` object to interact with GCS.
    *   Retrieves the specified bucket using `client.get_bucket(gcs_bucket_name)`.
    *   Creates a `blob` object to represent the file in GCS using `bucket.blob(gcs_blob_name)`.
    *   Uploads the local file to GCS using `blob.upload_from_filename(local_file_path)`.

7.  **`main()` Function:**
    *   Calls `download_dataset()` to download the Kaggle dataset.
    *   Constructs the full local file path and the GCS blob name.
    *   Calls `upload_to_gcs()` to upload the downloaded dataset file to GCS.

8.  **Main Execution Block:** Ensures the `main()` function is called when the script is executed.

### Next Steps:

1.  **Set up GCP Bucket:** Create a Google Cloud Storage bucket and ensure you have permissions to upload to it.
2.  **Replace Placeholders:** Replace the following with your actual values:
    *   `path_to_your_gcp_service_account_key.json`: The path to your GCP service account key file.
    *   `C:\\Users\\loydt\\Documents\\.kaggle`: The path to your Kaggle API credentials.
    *  `C:\\Users\\loydt\\Documents\\kaggle_datasets`: The directory to store the downloaded Kaggle data temporarily.
    *   `your-gcp-bucket-name`: The name of your Google Cloud Storage bucket.
3.  **Docker Containerization:** Containerize your ETL processing using Docker.
4.  **Deploy to GCP:** Deploy the Docker container to GCP using Cloud Functions, Cloud Run, or similar services.
5.  **Automation:** Automate the process using Cloud Scheduler or other scheduling tools.

This script is an important step to a fully cloud based data pipeline. Remember that this script assumes the structure of the downloaded kaggle data.

This comprehensive markdown document provides a clear explanation of the provided Python script, making it easier to understand and implement.

